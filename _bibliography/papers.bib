---
---

@string{aps = {American Physical Society,}}

@inproceedings{wang2025blossom,
abbr = {SIGGRAPH},
author = {Wang, Yingna and Liu, Qingqin and Wei, Xiaoying and Fanâ€ , Mingming},
title = {Blossoms Across Time: AI-Assisted Cultural Dialogue Through Diverse Artistic Expressions in VR Intangible Cultural Heritage Experience},
year = {2025},
isbn = {9798400715471}, 
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721245.3734052},
doi = {10.1145/3721245.3734052},
abstract = {Intangible Cultural Heritage (ICH) serves as a bridge between past, present, and future. For ICH traditions that have experienced historical disruptions, ancient texts and paintings provide invaluable insights. However, current VR systems for ICH often focus on static preservation, while AI applications remain largely limited to archival and documentation tasks. We present Timeless Blossoms, an AI-assisted VR system that enables practitioners to engage in realistic 3D flower arrangement while witnessing their Traditional Chinese Flower Arrangement (TCFA) compositions transform into 2D Xieyi (Chinese expressive) paintings in real time. By enabling participants to engage in a cross-temporal and cross-medium creative process, Timeless Blossoms offers a novel approach to cultural heritage preservation through AI and VR.  Our work advances the discourse on AI-assisted cultural creativity by demonstrating how computational techniques can bridge diverse artistic expressions within ICH experiences.},
booktitle = {ACM SIGGRAPH 2025 Immersive Pavilion},
articleno = {},
numpages = {2},
keywords = {Virtual Reality, Generative AI, Intangible Cultural Heritage, Traditional Chinese Flower Arrangement},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '25},
preview = {blossom-preview.jpg},
pdf = {Blossoms.pdf},
annotation={â€  corresponding author},
selected={true}
}

@inproceedings{liu2025virchew,
abbr = {SIGGRAPH},
author = {Liu, Qingqin and Fang, Ziqi and Wu, Jiayi and Cai, Shaoyu and Yan, Jianhui and Mo, Tiande and CHAN, Shuk Ching and Zhuâ€ , Kening},
title = {VirCHEW Reality: On-Face Kinesthetic Feedback for Enhancing Food-Intake Experience in Virtual Reality},
year = {2025},
isbn = {9798400715402}, 
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721238.3730694},
doi = {10.1145/3721238.3730694},
abstract = {While haptic interfaces for virtual reality (VR) has received extensive research attention, on-face haptics in VR remained less explored, especially for virtual food intake. In this paper, we introduce VirCHEW Reality, a face-worn haptic device designed to provide on-face kinesthetic force feedback, to enhance the virtual food-chewing experience in VR. Leveraging a pneumatic actuation system, VirCHEW Reality controlled the process of air inflation and deflation, to simulate the mechanical properties of food textures, such as hardness, cohesiveness, and stickiness. We evaluated the system through three user studies. First, a just-noticeable difference (JND) study examined usersâ€™ sensitivity to and the systemâ€™s capability of rendering different levels of on-face pneumatic-based kinesthetic feedback while users performing chewing action. Building upon the user-distinguishable signal ranges found in the first study, we further conducted a matching study to explore the correspondence between the kinesthetic stimuli provided by our device and user-perceived food textures, revealing the capability of simulating food texture properties during chewing (e.g., hardness, cohesiveness, stickiness). Finally, a user study in a VR eating scenario showed that VirCHEW Reality could significantly improve the usersâ€™ ratings on the sense of presence, compared to the condition without haptic feedback. Our findings further highlighted possible applications in virtual/remote dining, healthcare, and immersive entertainment.},
booktitle = {ACM SIGGRAPH 2025 Conference Papers},
articleno = {},
numpages = {},
keywords = {Virtual reality, Haptic devices, Food Texture},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '25},
preview = {VirCHEW-preview.jpg},
pdf = {VirCHEW-0514.pdf},
annotation={â€  corresponding author},
award = {Top 10 "Audience Choice" at SIGGRAPH 2025 Papers Fast Forward ðŸ¥³},
award_name = {Award},
selected={true}
}

@inproceedings{wang2025facilitating,
  abbr={ACM CHI},
  author = {Wang, Yingna and Liu, Qingqin and Wei, Xiaoying and Fanâ€ , Mingming},
  title = {Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement},
  year = {2025},
  isbn = {9798400713941},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3706598.3713409},
  doi = {10.1145/3706598.3713409},
  abstract = {The essence of intangible cultural heritage (ICH) lies in the living knowledge and skills passed down through generations. Daily practice plays a vital role in revitalizing ICH by fostering continuous learning and improvement. However, limited resources and accessibility pose significant challenges to sustaining such practice. Virtual reality (VR) has shown promise in supporting extensive skill training. Unlike technical skill training, ICH daily practice prioritizes cultivating a deeper understanding of cultural meanings and values. This study explores VR's potential in facilitating ICH daily practice through a case study of Traditional Chinese Flower Arrangement (TCFA). By investigating TCFA learners' challenges and expectations, we designed and evaluated FloraJing, a VR system enriched with cultural elements to support sustained TCFA practice. Findings reveal that FloraJing promotes progressive reflection, and continuous enhances technical improvement and cultural understanding. We further propose design implications for VR applications aimed at fostering ICH daily practice in both knowledge and skills.},
  booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  articleno = {326},
  numpages = {17},
  keywords = {Virtual Reality, Intangible Cultural Heritage, Daily Practice, Traditional Chinese Flower Arrangement},
  location = {Yokohama, Japan},
  series = {CHI '25},
  preview = {FloraJing-teaser.jpg},
  pdf = {FloraJing.pdf},
  annotation={â€  corresponding author},
  selected={true}
}

@article{xiong2024petpresence,
  abbr={IEEE TVCG},
  title={Petpresence: Investigating the integration of real-world pet activities in virtual reality},
  author={Xiong#, Ningchang and Liu#, Qingqin and Zhuâ€ , Kening},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  abstract = {For VR interaction, the home environment with complicated spatial setup and dynamics may hinder the VR user experience. In particular, pets' movement may be more unpredictable. In this paper, we investigate the integration of real-world pet activities into immersive VR interaction. Our pilot study showed that the active pet movements, especially dogs, could negatively impact users' performance and experience in immersive VR. We proposed three different types of pet integration, namely semitransparent real-world portal, non-interactive object in VR, and interactive object in VR. We conducted the user study with 16 pet owners and their pets. The results showed that compared to the baseline condition without any pet-integration technique, the approach of integrating the pet as interactive objects in VR yielded significantly higher participant ratings in perceived realism, joy, multisensory engagement, and connection with their pets in VR.},
  year={2024},
  publisher={IEEE},
  pdf = {PetPresence.pdf},
  html = {https://ieeexplore.ieee.org/document/10458353},
  preview = {PetVR-teaser.jpg},
  annotation={# contributed equally, â€  corresponding author},
  selected={true}
}

@inproceedings{zhang2022redirected,
	abbr={IEEE VR},
	title = {Redirected {Walking} in 360Â° {Video}: {Effect} of {Environment} {Size} on {Detection} {Thresholds} for {Translation} and {Rotation} {Gains}},
	shorttitle = {Redirected {Walking} in 360Â° {Video}},
	doi = {10.1109/VRW55335.2022.00266},
  html={https://ieeexplore.ieee.org/document/9757447},
	abstract = {Using real walking to control the playback of the 360Â° videos is a natural and immersive way to match visual and self-motion perception. Redirected walking can enable users to walk in limited physical tracking space but experience larger scenes. Environment size may affect user perception in 360Â° videos. We conducted a user study about the detection thresholds (DTs) for translation and rotation gains in 360Â° video-based virtual environments in three scenes with different widths. Results show that environment size of the scene increases the DTs for both lower and upper translation gains but doesn't affect the DTs for rotation gains.},
	booktitle = {2022 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	author = {Zhang, Yanxiang and Liu, Qingqin and Wang, Yingna},
	month = mar,
	year = {2022},
	keywords = {360Â° Video, Conferences, cybernetics, Design automation, Design methodologyâ€”Graphicsâ€”Virtual reality, Electronic design automation and methodology, Legged locomotion, Locomotion, man, Redirected Walking, Systems, Three-dimensional displays, User interfaces, User interfacesâ€”Human computer interactionâ€”Immersive experience, Virtual environments, Virtual Reality, Virtual Travel, Visualization},
	pages = {830--831},
  pdf = {pub-01.pdf},
  preview = {tour-360.png},
  selected={true}
}

@inproceedings{zhang2022touch,
	abbr={IEEE VR},
	title = {Touch the {History} in {Virtuality}: {Combine} {Passive} {Haptic} with 360Â° {Videos} in {History} {Learning}},
	shorttitle = {Touch the {History} in {Virtuality}},
	doi = {10.1109/VRW55335.2022.00263},
  html={https://ieeexplore.ieee.org/document/9757459},
	abstract = {History learning can help learners understand changes and establish identity. 360Â° videos can display complex history scenes, but limited to the visual and auditory. Passive haptic can provide tactile feedback by matching physical props to virtual counterparts. Utilizing passive haptic in 360Â° videos can create a diverse and rich experience but face matching problems because of perspective distortion. We propose using high-fidelity models and mapping videos to a cube space to solve this problem. The results of a 28 participants user study show that combining passive haptic with 360Â° videos enhances users' presence in historical experience.},
	booktitle = {2022 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	author = {Zhang, Yanxiang and Wang, Yingna and Liu, Qingqin},
	month = mar,
	year = {2022},
	keywords = {[Computers and Information Processing]: Haptic Interfaces, [Systems, Man, and Cybernetics]: Human Computer Interaction-Immersive Experience, 360Â° videos, Computational modeling, Conferences, Distortion, Haptic interfaces, history learning, passive haptic, Solid modeling, space match, Virtual reality, Visualization},
	pages = {824--825},
  pdf = {pub-02.pdf},
  preview = {touch-360.png},
  selected={true}
}

@inproceedings{zhang2022sloped,
	abbr={IEEE VR},
	title = {The {Sloped} {Shoes}: {Influence} {Human} {Perception} of the {Virtual} {Slope}},
	shorttitle = {The {Sloped} {Shoes}},
	doi = {10.1109/VRW55335.2022.00264},
  html={https://ieeexplore.ieee.org/document/9757641},
	abstract = {At present, there is little research on redirected walking in the direction of the slope. In this study, people were allowed to walk uphill or downhill in virtual environments by changing the slope of users' shoes, while they walked on a flat wooden board in physical environments. We can explore the impact of the shoe slope on users' perception of walking uphill or downhill in the virtual world. We find that the slope of the shoes affects participants' perception, increasing their sense of realism when walking uphill and downhill in the virtual world. With increasing or decreasing the shoe slope, participants' perception of the possibility of walking uphill or downhill will also change and establish corresponding detection thresholds.},
	booktitle = {2022 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	author = {Zhang, Yanxiang and Wu, Jialing and Liu, Qingqin},
	month = mar,
	year = {2022},
	keywords = {Conferences, detection threshold, Footwear, Legged locomotion, perception, redirected walking, shoe slope, Three-dimensional displays, User experience, User interfaces, Virtual environments, Virtual reality, Virtual reality and Human computer interaction-haptic feedback-perception},
	pages = {826--827},
  pdf = {pub-03.pdf},
  preview = {sloped-shoes.png},
  selected={true}
}
